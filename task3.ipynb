{
 "metadata": {
  "name": "",
  "signature": "sha256:519441c881f4bbfd750fa134b500083076deba9e5e60d057843f98931e137cad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "\n",
      "\n",
      "def iter_docs(path):\n",
      "    for file_ in os.listdir(path):\n",
      "        handle = open(os.path.join(path, file_), 'r')\n",
      "        doc = handle.read()\n",
      "        if doc:\n",
      "            yield doc\n",
      "\n",
      "\n",
      "def letters_only(word):\n",
      "    letters = [char for char in word if char.isalpha()]\n",
      "    return ''.join(letters)\n",
      "\n",
      "\n",
      "def stop_words():\n",
      "    handle = open('/home/jamsic/python2/task3/SentenceCorpus/word_lists/stopwords.txt', 'r')\n",
      "    stop_words_list = []\n",
      "    for line in handle:\n",
      "        stop_words_list.append(line.strip())\n",
      "    return stop_words_list\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 195
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import marisa_trie\n",
      "import numpy as np\n",
      "from Stemmer import Stemmer\n",
      "from tabulate import tabulate\n",
      "\n",
      "\n",
      "class TrieVectorizer:\n",
      "\n",
      "    def __init__(self):\n",
      "        self._vocabulary = None\n",
      "        self._stemmer = Stemmer(\"english\")\n",
      "\n",
      "    def fit(self, docs):\n",
      "        all_words = []\n",
      "        for doc in docs:\n",
      "            #print('doc', doc)\n",
      "            all_words += [token for token in self._process(doc)]\n",
      "        #print('all_words', all_words)\n",
      "        self._vocabulary = marisa_trie.Trie(all_words)\n",
      "        return self\n",
      "\n",
      "    def transform(self, docs):\n",
      "        matrix = []\n",
      "        for doc in docs:\n",
      "            lst = np.zeros(len(self._vocabulary))\n",
      "            for token in self._process(doc):\n",
      "                #print(token)\n",
      "                #print(self._vocabulary.keys())\n",
      "                if token in self._vocabulary.keys():\n",
      "                    lst[self._vocabulary[token]] += 1\n",
      "            matrix.append(lst)\n",
      "        return np.array(matrix)\n",
      "\n",
      "    def fit_transform(self, docs):\n",
      "        return self.fit(docs).transform(docs)\n",
      "\n",
      "    def _process(self, doc):\n",
      "        return self._tokenize(self._analyze(doc))\n",
      "\n",
      "    def _analyze(self, doc):\n",
      "        # \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0441\u043b\u043e\u0432 \u0438\u0437 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\n",
      "        for word in doc.split():\n",
      "            yield word.lower()\n",
      "\n",
      "    def _tokenize(self, words, min_len=3):\n",
      "        stop_words_list = stop_words()\n",
      "        for word in words:\n",
      "            token = self._stemmer.stemWord(letters_only(word))\n",
      "            if token not in stop_words_list:\n",
      "                yield unicode(token)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 196
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import itemfreq\n",
      "\n",
      "\n",
      "class NaiveBayes:\n",
      "    def fit(self, X, y):   # time: 0.74035 s 0.047336 s\n",
      "        n_docs, n_words = len(X), len(X[0])\n",
      "        n_classes = 5\n",
      "        #print type(X), type(y)\n",
      "\n",
      "        epsilon = np.finfo(float).eps\n",
      "        #X = np.array(X)\n",
      "        #y = np.array(y)\n",
      "        count = itemfreq(y)\n",
      "        #print count, type(count), count[0]\n",
      "        class_count = []\n",
      "        for i in range(n_classes):\n",
      "            #print count[i], count[i][1]\n",
      "            class_count.append(count[i][1])\n",
      "        #class_char_fun = np.array([~y, y])  !!!\n",
      "        class_char_fun = []\n",
      "        for i in range(n_classes):\n",
      "            ls = []\n",
      "            for sym in y:\n",
      "                if sym == i:\n",
      "                    ls.append(1)\n",
      "                else:\n",
      "                    ls.append(0)\n",
      "            class_char_fun.append(ls)\n",
      "        class_count = np.array(class_count)\n",
      "        class_char_fun = np.array(class_char_fun)\n",
      "        #print type(X)\n",
      "        #print X.shape, class_count.shape, class_char_fun.shape\n",
      "        #print class_count\n",
      "        class_prob = class_count /1./ n_docs\n",
      "        feature_prob = np.dot(class_char_fun, X) + epsilon\n",
      "        feature_prob = feature_prob/feature_prob.sum(axis=0)\n",
      "        self._class_prob = class_prob\n",
      "        self._feature_prob = feature_prob\n",
      "\n",
      "        return self\n",
      "\n",
      "    def predict(self, X):  # 21.6416 s 0.052417 s\n",
      "        n_classes = 5\n",
      "\n",
      "        X = np.array(X)\n",
      "        math_log_prob = np.array([np.log(self._feature_prob[c])\n",
      "                                 for c in range(n_classes)])\n",
      "        log_prob = np.dot(math_log_prob, X.T)\n",
      "        log_class = np.array([np.full(log_prob.shape[1],\n",
      "                                      np.log(self._class_prob[c]))\n",
      "                              for c in range(n_classes)])\n",
      "        log_prob += log_class\n",
      "        y = np.argmax(log_prob, axis=0)\n",
      "        return y\n",
      "\n",
      "\n",
      "def train_test_split(X, y, ratio=0.2):\n",
      "    mask = np.random.binomial(1, ratio, len(X)) == 1\n",
      "    return X[mask], X[~mask], y[mask], y[~mask]\n",
      "\n",
      "\n",
      "def classification_report(y_true, y_pred, labels):\n",
      "    def precision_recall_total(y_true, y_pred):\n",
      "        tp = (y_pred * y_true).sum()\n",
      "        fp = (y_pred * ~y_true).sum()\n",
      "        fn = (~y_pred * y_true).sum()\n",
      "        precision = tp /1./ (tp + fp)\n",
      "        recall = tp /1./ (tp + fn)\n",
      "        return precision, recall, int(y_true.sum())\n",
      "\n",
      "    acc = []\n",
      "    for c, label in enumerate(labels):\n",
      "        precision, recall, total = precision_recall_total(\n",
      "            np.asarray(y_true) == c, np.asarray(y_pred) == c)\n",
      "        acc.append([label, precision, recall, total])\n",
      "\n",
      "    print(tabulate(acc, headers=[\"\", \"precision\", \"recall\", \"total\"],\n",
      "                   floatfmt=\".2f\"))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 197
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dir_ = '/home/jamsic/python2/task3/SentenceCorpus/labeled_articles'\n",
      "words = list(iter_docs(dir_))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 198
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def split_on_classes(docs):\n",
      "    classes = []\n",
      "    sentences = []\n",
      "    for doc in docs:\n",
      "        sents = doc.split('\\n')\n",
      "        for sent in sents:\n",
      "            if sent.startswith(('MISC', 'OWNX', 'CONT', 'AIMX', 'BASE')):\n",
      "                cl = sent[:4]\n",
      "                #print(cl, len(cl))\n",
      "                if cl == 'MISC':\n",
      "                    classes.append(0)\n",
      "                if cl == 'OWNX':\n",
      "                    classes.append(1)\n",
      "                if cl == 'CONT':\n",
      "                    classes.append(2)\n",
      "                if cl == 'AIMX':\n",
      "                    classes.append(3)\n",
      "                if cl == 'BASE':\n",
      "                    classes.append(4)\n",
      "                sentences.append(sent[5:])\n",
      "    return np.array(classes), np.array(sentences)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 199
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a, b = split_on_classes(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 200
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs_train, docs_test, y_train, y_test = train_test_split(b, a)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 201
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "v = TrieVectorizer()\n",
      "X_train = v.fit_transform(docs_train)\n",
      "X_test = v.transform(docs_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 202
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 203,
       "text": [
        "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       ..., \n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
        "       [ 3.,  0.,  0., ...,  0.,  0.,  0.]])"
       ]
      }
     ],
     "prompt_number": 203
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = NaiveBayes().fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 204
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = np.array(clf.predict(X_test))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 205
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 206,
       "text": [
        "array([0, 0, 1, ..., 1, 1, 1])"
       ]
      }
     ],
     "prompt_number": 206
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(y_pred == y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 207,
       "text": [
        "1802"
       ]
      }
     ],
     "prompt_number": 207
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(y_pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 208,
       "text": [
        "2486"
       ]
      }
     ],
     "prompt_number": 208
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sum(y_pred == y_test)/1./len(y_pred)  ### \u041e\u0431\u0449\u0430\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 209,
       "text": [
        "0.72485921158487532"
       ]
      }
     ],
     "prompt_number": 209
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classification_report(y_test, y_pred, ['MISC', 'OWNX', 'CONT', 'AIMX', 'BASE'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "        precision    recall    total\n",
        "----  -----------  --------  -------\n",
        "MISC         0.73      0.95     1465\n",
        "OWNX         0.74      0.48      681\n",
        "CONT         0.56      0.19      134\n",
        "AIMX         0.75      0.29      156\n",
        "BASE         0.61      0.22       50\n"
       ]
      }
     ],
     "prompt_number": 210
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}